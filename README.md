# ETL-Project
Extract:
We found our Bigfoot Reports dataset on data.world. It consisted of two CSV files, a location file and a geocoded file, and one JSON file. The locations CSV file contained only six columns and 3994 rows. The geocoded CSV file contained 28 columns and 4747 rows. The JSON file contained 18 columns, but it also had 4747 rows.
 
Transform:
Initially, we read all three files into a Jupyter Notebook. They all seemed to contain similar data, with, at the very least, case numbers, some form of date, classification, and a location.
We began transforming the data by dropping duplicates (of which there were none), then dropping rows of data if the case number was null. Only the JSON file contained data with null case numbers.
Once the null values had been dropped, we cut columns in the dataframes in order to focus on the most consistent data available:
·        For the location dataframe, we kept the ‘number’ (case number), ‘classification’, ‘timestamp', 'latitude', and 'longitude' columns. While the other two datasets contained a ‘county’ and ‘state’ for each report, the location dataset only recorded the latitude and longitude. We wanted to have a location available for each report, so we chose to keep the latitude and longitude in case a case number did not have a county and state recorded in the other two datasets. In addition, the ‘timestamp’ column for the location dataset was done in a <DATE>T<TIME> format (ex. ‘2000-06-16T12:00:00Z’). However, the time for each report was recorded as 12:00:00. Because of this, we split the timestamp column so that we had a date column and a time column, then we dropped the time column. Having only the date would also allow us to compare the recorded date in the location dataset to the recorded date in the geocoded and JSON datasets.
·        With the geocoded dataset, there was an extensive amount of information recorded, including ‘moon_phase’, ‘uv_index’, ‘wind_speed’, etc. Most of those columns contained null values for each case, so we only chose to keep 'observed', 'county', 'state', 'latitude', 'longitude', 'date', 'number', and 'classification’ columns. We kept the latitude and longitude columns to compare to the location dataset. The date format of the geocoded dataset was the same as the split date format of the location dataset.
·        Finally, we cut the JSON dataset so that we only had the 'OBSERVED', 'COUNTY', 'STATE', 'YEAR', 'MONTH', 'REPORT_NUMBER', and 'REPORT_CLASS' columns. All column titles in the JSON dataset were capitalized, the case number (named ‘number’ in the geocoded and location datasets) was named ‘REPORT_NUMBER’, and the classification (named ‘classification in the geocoded and location datasets) was named ‘REPORT_CLASS’. This dataset also did not have a ‘date’ column formatted in the same way as the geocoded and the location datasets. Instead, it contained ‘year’, ‘month’, and ‘date’ (day) columns. In addition to this difference, the data recorded for each of those columns was not consistent. Rather than having a specific year, some columns contained a string such as ‘early 1990s’, and some rows in the ‘date’ column contained multiple days. Because of this, we only chose to keep the month and year columns. After focusing on the selected columns, we renamed the columns so that they were formatted and titled to match the location and geocoded datasets.
After cleaning the datasets individually, we merged the tables. First, we merged the JSON and the geocoded datasets, because they both contained 4747 rows. We used an outer merge of the two datasets on the ‘number’ column, and after merging, we still only had 4747 rows. This suggests that both datasets contained the same case numbers, and, hopefully, also the same observations, classifications, locations, and dates.
Load:

Now that we have all the individual data sets cleaned up: bigfoot reports, locations, and geocodes It was fairly simple to load them into a postgres database as tables. The main goal for sql was to merge all the individual tables into a master table with all of the unique report numbers, their respective classification, and a potential timestamp if possible. 

